{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LXMLS 2017 - Day 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This day will serve as an introduction to machine learning. We recall some fundamental concepts about decision\n",
    "theory and classification. We also present some widely used models and algorithms and try to provide\n",
    "the main motivation behind them. There are several textbooks that provide a thorough description of some\n",
    "of the concepts introduced here: for example, Mitchell (1997), Duda et al. (2001), Scholkopf and Smola (2002), Â¨\n",
    "Joachims (2002), Bishop (2006), Manning et al. (2008), to name just a few. The concepts that we introduce in this\n",
    "chapter will be revisited in later chapters, where the same algorithms and models will be adapted to structured\n",
    "inputs and outputs. For now, we concern only with multi-class classification (with just a few classes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.1** In this exercise we will use the Amazon sentiment analysis data (Blitzer et al., 2007), where the goal is to classify text documents as expressing a positive or negative sentiment (i.e., a classification problem with two classes).We are going to focus on book reviews. To load the data, type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "1600\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../../../')\n",
    "import pdb\n",
    "\n",
    "\n",
    "import lxmls.readers.simple_data_set as sds\n",
    "import lxmls.readers.sentiment_reader as srs\n",
    "import lxmls.classifiers.linear_classifier as lcc\n",
    "import lxmls.classifiers.perceptron as percc\n",
    "import lxmls.classifiers.mira as mirac\n",
    "import lxmls.classifiers.gaussian_naive_bayes as gnbc\n",
    "import lxmls.classifiers.multinomial_naive_bayes as mnb\n",
    "\n",
    "reload(mnb)  # this allows you to edit the module and run this script again without rebooting Python\n",
    "import lxmls.classifiers.max_ent_batch as mebc\n",
    "import lxmls.classifiers.max_ent_online as meoc\n",
    "import lxmls.classifiers.svm as svmc\n",
    "import lxmls.readers.sentiment_reader as srs\n",
    "\n",
    "scr = srs.SentimentCorpus(\"books\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resolution of the exercise 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(self, x, y):\n",
    "    # n_docs = no. of documents\n",
    "    # n_words = no. of unique words\n",
    "    n_docs, n_words = x.shape\n",
    "\n",
    "    # classes = a list of possible classes\n",
    "    classes = np.unique(y)\n",
    "    # n_classes = no. of classes\n",
    "    n_classes = np.unique(y).shape[0]\n",
    "\n",
    "    # initialization of the prior and likelihood variables\n",
    "    prior = np.zeros(n_classes)\n",
    "    likelihood = np.zeros((n_words, n_classes))\n",
    "\n",
    "    # TODO: This is where you have to write your code!\n",
    "    # You need to compute the values of the prior and likelihood parameters\n",
    "    # and place them in the variables called \"prior\" and \"likelihood\".\n",
    "    # Examples:\n",
    "    # prior[0] is the prior probability of a document being of class 0\n",
    "    # likelihood[4, 0] is the likelihood of the fifth(*) feature being\n",
    "    # active, given that the document is of class 0\n",
    "    # (*) recall that Python starts indices at 0, so an index of 4\n",
    "    # corresponds to the fifth feature!\n",
    "\n",
    "    ##########################\n",
    "    # Solution to Exercise 1.1\n",
    "    ##########################\n",
    "    for i in xrange(n_classes):\n",
    "        docs_in_class, _ = np.nonzero(y == classes[i])  # docs_in_class = indices of documents in class i\n",
    "        prior[i] = 1.0 * len(docs_in_class) / n_docs  # prior = fraction of documents with this class\n",
    "\n",
    "        # word_count_in_class = count of word occurrences in documents of class i\n",
    "        word_count_in_class = x[docs_in_class, :].sum(0)\n",
    "        total_words_in_class = word_count_in_class.sum()  # total_words_in_class = total number of words in documents of class i\n",
    "        if not self.smooth:\n",
    "            # likelihood = count of occurrences of a word in a class\n",
    "            likelihood[:, i] = word_count_in_class / total_words_in_class\n",
    "        else:\n",
    "            likelihood[:, i] = (word_count_in_class+self.smooth_param) / (total_words_in_class + self.smooth_param*n_words)\n",
    "    ##############################\n",
    "    # End solution to Exercise 1.1\n",
    "    ##############################\n",
    "\n",
    "    params = np.zeros((n_words+1, n_classes))\n",
    "    for i in xrange(n_classes):\n",
    "        params[0, i] = np.log(prior[i])\n",
    "        params[1:, i] = np.nan_to_num(np.log(likelihood[:, i]))\n",
    "    self.likelihood = likelihood\n",
    "    self.prior = prior\n",
    "    self.trained = True\n",
    "    return params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes Amazon Sentiment Accuracy train: 0.974375 test: 0.840000\n"
     ]
    }
   ],
   "source": [
    "import lxmls.classifiers.multinomial_naive_bayes as mnbb\n",
    "mnb = mnbb.MultinomialNaiveBayes()\n",
    "params_nb_sc = mnb.train(scr.train_X,scr.train_y)\n",
    "y_pred_train = mnb.test(scr.train_X,params_nb_sc)\n",
    "acc_train = mnb.evaluate(scr.train_y, y_pred_train)\n",
    "y_pred_test = mnb.test(scr.test_X,params_nb_sc)\n",
    "acc_test = mnb.evaluate(scr.test_y, y_pred_test)\n",
    "print \"Multinomial Naive Bayes Amazon Sentiment Accuracy train: %f test: %f\"%(\n",
    "acc_train,acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.69314718 -1.69314718]\n",
      " [-1.          1.        ]\n",
      " [-1.          1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1.1: run all classifiers on 2D data ####\n",
    "\n",
    "# This instruction generates a simple 2D dataset with two classes.\n",
    "# Each class is a Gaussian distribution.\n",
    "# Input parameters (feel free to change them):\n",
    "# nr_examples: number of points in the dataset\n",
    "# g1: parameters for the first gaussian, of the form:\n",
    "# g1 = [[mean_x,mean_y], std]\n",
    "# mean_x and mean_y are the x and y coordinates of the mean of the Gaussian\n",
    "# std is the standard deviation of the Gaussian\n",
    "# g2: parameters for the second gaussian, with a similar form as g1\n",
    "# balance: percentage of points in the first gaussian\n",
    "# split: fraction of points to use for train, development, and test respectively\n",
    "sd = sds.SimpleDataSet(nr_examples=100,\n",
    "                       g1=[[-1, -1], 1],\n",
    "                       g2=[[1, 1], 1],\n",
    "                       balance=0.5,\n",
    "                       split=[0.5, 0, 0.5])\n",
    "\n",
    "# Plot the data and the Bayes Optimal classifier\n",
    "fig, axis = sd.plot_data()\n",
    "\n",
    "# Initialize the Naive Bayes (NB) classifier for Gaussian data\n",
    "gnb = gnbc.GaussianNaiveBayes()\n",
    "\n",
    "# Learn the NB parameters from the train data\n",
    "params_nb_sd = gnb.train(sd.train_X, sd.train_y)\n",
    "\n",
    "# Use the learned parameters to predict labels for the training data\n",
    "y_pred_train = gnb.test(sd.train_X, params_nb_sd)\n",
    "\n",
    "# Compute accuracy on training data from predicted labels and true labels\n",
    "acc_train = gnb.evaluate(sd.train_y, y_pred_train)\n",
    "\n",
    "# Use the learned parameters to predict labels for the test data\n",
    "y_pred_test = gnb.test(sd.test_X, params_nb_sd)\n",
    "\n",
    "# Compute accuracy on test data from predicted labels and true labels\n",
    "acc_test = gnb.evaluate(sd.test_y, y_pred_test)\n",
    "\n",
    "# Add a line to the plot with the line corresponding to the NB classifier\n",
    "fig, axis = sd.add_line(fig, axis, params_nb_sd, \"Naive Bayes\", \"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Simple Dataset Accuracy train: 0.900000 test: 0.840000\n",
      "\n",
      "Rounds: 0 Accuracy: 0.900000\n",
      "Rounds: 1 Accuracy: 0.780000\n",
      "Rounds: 2 Accuracy: 0.880000\n",
      "Rounds: 3 Accuracy: 0.860000\n",
      "Rounds: 4 Accuracy: 0.820000\n",
      "Rounds: 5 Accuracy: 0.860000\n",
      "Rounds: 6 Accuracy: 0.900000\n",
      "Rounds: 7 Accuracy: 0.860000\n",
      "Rounds: 8 Accuracy: 0.900000\n",
      "Rounds: 9 Accuracy: 0.820000\n",
      "Perceptron Simple Dataset Accuracy train: 0.900000 test: 0.820000\n",
      "\n",
      "Rounds: 0 Accuracy: 0.880000\n",
      "Rounds: 1 Accuracy: 0.900000\n",
      "Rounds: 2 Accuracy: 0.680000\n",
      "Rounds: 3 Accuracy: 0.760000\n",
      "Rounds: 4 Accuracy: 0.680000\n",
      "Rounds: 5 Accuracy: 0.820000\n",
      "Rounds: 6 Accuracy: 0.840000\n",
      "Rounds: 7 Accuracy: 0.840000\n",
      "Rounds: 8 Accuracy: 0.320000\n",
      "Rounds: 9 Accuracy: 0.880000\n",
      "Mira Simple Dataset Accuracy train: 0.860000 test: 0.800000\n",
      "\n",
      "Objective = 0.69314718056\n",
      "Objective = 0.820050382317\n",
      "Objective = 0.518005699926\n",
      "Objective = 0.517603782644\n",
      "Objective = 0.517541454694\n",
      "Objective = 0.517541361737\n",
      "Objective = 0.517541360472\n",
      "Max-Ent batch Simple Dataset Accuracy train: 0.900000 test: 0.820000\n",
      "\n",
      "Epochs: 0 Objective: 0.633231\n",
      "Epochs: 0 Accuracy: 0.900000\n",
      "Epochs: 1 Objective: 0.524032\n",
      "Epochs: 1 Accuracy: 0.900000\n",
      "Epochs: 2 Objective: 0.521010\n",
      "Epochs: 2 Accuracy: 0.900000\n",
      "Epochs: 3 Objective: 0.519924\n",
      "Epochs: 3 Accuracy: 0.900000\n",
      "Epochs: 4 Objective: 0.519359\n",
      "Epochs: 4 Accuracy: 0.900000\n",
      "Epochs: 5 Objective: 0.519012\n",
      "Epochs: 5 Accuracy: 0.900000\n",
      "Epochs: 6 Objective: 0.518776\n",
      "Epochs: 6 Accuracy: 0.900000\n",
      "Epochs: 7 Objective: 0.518606\n",
      "Epochs: 7 Accuracy: 0.900000\n",
      "Epochs: 8 Objective: 0.518477\n",
      "Epochs: 8 Accuracy: 0.900000\n",
      "Epochs: 9 Objective: 0.518376\n",
      "Epochs: 9 Accuracy: 0.900000\n",
      "Max-Ent Online Simple Dataset Accuracy train: 0.900000 test: 0.820000\n",
      "\n",
      "Epochs: 0 Objective: 0.734302\n",
      "Epochs: 0 Accuracy: 0.900000\n",
      "Epochs: 1 Objective: 0.504877\n",
      "Epochs: 1 Accuracy: 0.920000\n",
      "Epochs: 2 Objective: 0.494082\n",
      "Epochs: 2 Accuracy: 0.900000\n",
      "Epochs: 3 Objective: 0.491509\n",
      "Epochs: 3 Accuracy: 0.900000\n",
      "Epochs: 4 Objective: 0.489225\n",
      "Epochs: 4 Accuracy: 0.900000\n",
      "Epochs: 5 Objective: 0.487800\n",
      "Epochs: 5 Accuracy: 0.900000\n",
      "Epochs: 6 Objective: 0.486826\n",
      "Epochs: 6 Accuracy: 0.900000\n",
      "Epochs: 7 Objective: 0.486117\n",
      "Epochs: 7 Accuracy: 0.900000\n",
      "Epochs: 8 Objective: 0.485579\n",
      "Epochs: 8 Accuracy: 0.900000\n",
      "Epochs: 9 Objective: 0.485155\n",
      "Epochs: 9 Accuracy: 0.900000\n",
      "SVM Online Simple Dataset Accuracy train: 0.900000 test: 0.820000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print these two accuracies to the terminal\n",
    "print \"Naive Bayes Simple Dataset Accuracy train: %f test: %f\" % (acc_train, acc_test)\n",
    "print\n",
    "\n",
    "# Same as above, but for the perceptron classifier (instead of Naive Bayes)\n",
    "perc = percc.Perceptron()\n",
    "params_perc_sd = perc.train(sd.train_X, sd.train_y)\n",
    "y_pred_train = perc.test(sd.train_X, params_perc_sd)\n",
    "acc_train = perc.evaluate(sd.train_y, y_pred_train)\n",
    "y_pred_test = perc.test(sd.test_X, params_perc_sd)\n",
    "acc_test = perc.evaluate(sd.test_y, y_pred_test)\n",
    "fig, axis = sd.add_line(fig, axis, params_perc_sd, \"Perceptron\", \"blue\")\n",
    "print \"Perceptron Simple Dataset Accuracy train: %f test: %f\" % (acc_train, acc_test)\n",
    "print\n",
    "\n",
    "# Same as above, but for the MIRA classifier\n",
    "mira = mirac.Mira()\n",
    "params_mira_sd = mira.train(sd.train_X, sd.train_y)\n",
    "y_pred_train = mira.test(sd.train_X, params_mira_sd)\n",
    "acc_train = mira.evaluate(sd.train_y, y_pred_train)\n",
    "y_pred_test = mira.test(sd.test_X, params_mira_sd)\n",
    "acc_test = mira.evaluate(sd.test_y, y_pred_test)\n",
    "fig, axis = sd.add_line(fig, axis, params_mira_sd, \"Mira\", \"green\")\n",
    "print \"Mira Simple Dataset Accuracy train: %f test: %f\" % (acc_train, acc_test)\n",
    "print\n",
    "\n",
    "# Same as above, but for the Maximum Entropy classifier, batch version\n",
    "me_lbfgs = mebc.MaxEntBatch()\n",
    "params_meb_sd = me_lbfgs.train(sd.train_X, sd.train_y)\n",
    "y_pred_train = me_lbfgs.test(sd.train_X, params_meb_sd)\n",
    "acc_train = me_lbfgs.evaluate(sd.train_y, y_pred_train)\n",
    "y_pred_test = me_lbfgs.test(sd.test_X, params_meb_sd)\n",
    "acc_test = me_lbfgs.evaluate(sd.test_y, y_pred_test)\n",
    "fig, axis = sd.add_line(fig, axis, params_meb_sd, \"Max-Ent-Batch\", \"orange\")\n",
    "print \"Max-Ent batch Simple Dataset Accuracy train: %f test: %f\" % (acc_train, acc_test)\n",
    "print\n",
    "\n",
    "# Same as above, but for the Maximum Entropy classifier, online version\n",
    "me_sgd = meoc.MaxEntOnline()\n",
    "params_meo_sd = me_sgd.train(sd.train_X, sd.train_y)\n",
    "y_pred_train = me_sgd.test(sd.train_X, params_meo_sd)\n",
    "acc_train = me_sgd.evaluate(sd.train_y, y_pred_train)\n",
    "y_pred_test = me_sgd.test(sd.test_X, params_meo_sd)\n",
    "acc_test = me_sgd.evaluate(sd.test_y, y_pred_test)\n",
    "fig, axis = sd.add_line(fig, axis, params_meo_sd, \"Max-Ent-Online\", \"magenta\")\n",
    "print \"Max-Ent Online Simple Dataset Accuracy train: %f test: %f\" % (acc_train, acc_test)\n",
    "print\n",
    "\n",
    "# Same as above, but for the SVM classifier\n",
    "svm = svmc.SVM()\n",
    "params_svm_sd = svm.train(sd.train_X, sd.train_y)\n",
    "y_pred_train = svm.test(sd.train_X, params_svm_sd)\n",
    "acc_train = svm.evaluate(sd.train_y, y_pred_train)\n",
    "y_pred_test = svm.test(sd.test_X, params_svm_sd)\n",
    "acc_test = svm.evaluate(sd.test_y, y_pred_test)\n",
    "fig, axis = sd.add_line(fig, axis, params_svm_sd, \"SVM\", \"yellow\")\n",
    "print \"SVM Online Simple Dataset Accuracy train: %f test: %f\" % (acc_train, acc_test)\n",
    "print\n",
    "\n",
    "# End of exercise 3.1 #########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.2** We provide an implementation of the perceptron algorithm in the class Perceptron (file perceptron.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(self, x, y, seed=1):\n",
    "    self.params_per_round = []\n",
    "    x_orig = x[:, :]\n",
    "    x = self.add_intercept_term(x)\n",
    "    nr_x, nr_f = x.shape\n",
    "    nr_c = np.unique(y).shape[0]\n",
    "    w = np.zeros((nr_f, nr_c))\n",
    "    for epoch_nr in xrange(self.nr_epochs):\n",
    "\n",
    "        # use seed to generate permutation\n",
    "        np.random.seed(seed)\n",
    "        perm = np.random.permutation(nr_x)\n",
    "\n",
    "        # change the seed so next epoch we don't get the same permutation\n",
    "        seed += 1\n",
    "\n",
    "        for nr in xrange(nr_x):\n",
    "            # print \"iter %i\" %( epoch_nr*nr_x + nr)\n",
    "            inst = perm[nr]\n",
    "            y_hat = self.get_label(x[inst:inst+1, :], w)\n",
    "\n",
    "            if y[inst:inst+1, 0] != y_hat:\n",
    "                # Increase features of th e truth\n",
    "                w[:, y[inst:inst+1, 0]] += self.learning_rate * x[inst:inst+1, :].transpose()\n",
    "\n",
    "                # Decrease features of the prediction\n",
    "                w[:, y_hat] += -1 * self.learning_rate * x[inst:inst+1, :].transpose()\n",
    "\n",
    "        self.params_per_round.append(w.copy())\n",
    "        self.trained = True\n",
    "        y_pred = self.test(x_orig, w)\n",
    "        acc = self.evaluate(y, y_pred)\n",
    "        self.trained = False\n",
    "        print \"Rounds: %i Accuracy: %f\" % (epoch_nr, acc)\n",
    "    self.trained = True\n",
    "\n",
    "    if self.averaged:\n",
    "        new_w = 0\n",
    "        for old_w in self.params_per_round:\n",
    "            new_w += old_w\n",
    "        new_w /= len(self.params_per_round)\n",
    "        return new_w\n",
    "    return w\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
