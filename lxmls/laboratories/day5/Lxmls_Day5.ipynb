{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LXMLS 2017 - Day 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning is the name behind the latest wave of neural network research. This is a very old topic, dating\n",
    "from the first half of the 20th century, that has attained formidable impact in the machine learning community\n",
    "recently. There is nothing particularly difficult in deep learning. You have already visited all the mathematical\n",
    "principles you need in the first days of the labs of this school. At their core, deep learning models are\n",
    "just functions mapping vector inputs x to vector outputs y, constructed by composing linear and non-linear\n",
    "functions. This composition can be expressed in the form of a computation graph, where each node applies a\n",
    "function to its inputs and passes the result as its output. The parameters of the model are the weights given\n",
    "to the different inputs in nodes applying linear functions. This vaguely resembles synapse strengths in human\n",
    "neural networks, hence the name artificial neural networks.\n",
    "\n",
    "Due to their compositional nature, gradient methods and the chain rule can be applied learn the parameters\n",
    "of these models regardless of their complexity. See Section for a refresh on the basic concept. We will also refer\n",
    "to the gradient learning methods introduced in Section 1.4.4. Today we will focus on feed-forward networks.\n",
    "Tomorrow we will extended todayâ€™s class to recurrent neural networks (RNNs).\n",
    "\n",
    "Some of the changes that led to the surge of deep learning are not only improvements on the existing neural\n",
    "network algorithms, but also the increase in the amount of data available and computing power. In particular,\n",
    "the use of Graphical Processing Units (GPUs) has allowed neural networks to be applied to very large datasets.\n",
    "Working with GPUs is not trivial as it requires dealing with specialized hardware. Luckily, as it is often the\n",
    "case, we are one Python import away from solving this problem.\n",
    "For the particular case of deep learning, there is a growing number of python toolboxes available that allow\n",
    "you to design custom computational graphs for GPUs as e.g. Theano1 or TensorFlow2\n",
    ".\n",
    "\n",
    "In these labs we will be working with Theano. Theano allows us to express computation graphs symbolically\n",
    "in terms of basic algebraic operations. It also automatically computes gradients and produces CUDAcompatible\n",
    "code for GPUs. The exercises are designed to gain a low-level understanding of Theano. If you\n",
    "are only looking forward to utilize pre-designed models, the Keras toolbox3 provides high-level operations\n",
    "compatible with both Theano and TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 5.1** - Start by loading Amazon sentiment corpus used in day 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "1600\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../../../')\n",
    "import numpy as np\n",
    "import lxmls.readers.sentiment_reader as srs\n",
    "scr = srs.SentimentCorpus(\"books\")\n",
    "train_x = scr.train_X.T\n",
    "train_y = scr.train_y[:, 0]\n",
    "test_x = scr.test_X.T\n",
    "test_y = scr.test_y[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to **lxmls/deep learning/mlp.py:class NumpyMLP:def grads()** and complete the code of the NumpyMLP class with\n",
    "the Backpropagation recursion that we just saw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def grads(self, x, y):\n",
    "        \"\"\"\n",
    "       Computes the gradients of the network with respect to cross entropy\n",
    "       error cost\n",
    "       \"\"\"\n",
    "\n",
    "        # Run forward and store activations for each layer\n",
    "        activations = self.forward(x, all_outputs=True)\n",
    "\n",
    "        # For each layer in reverse store the gradients for each parameter\n",
    "        nabla_params = [None] * (2*self.n_layers)\n",
    "\n",
    "        for n in np.arange(self.n_layers-1, -1, -1):\n",
    "\n",
    "            # Get weigths and bias (always in even and odd positions)\n",
    "            # Note that sometimes we need the weight from the next layer\n",
    "            W = self.params[2*n]\n",
    "            b = self.params[2*n+1]\n",
    "            if n != self.n_layers-1:\n",
    "                W_next = self.params[2*(n+1)]\n",
    "            \n",
    "            ######################\n",
    "            # Solving Exercise 5.1\n",
    "            ######################\n",
    "            if n < self.n_layers - 1 :\n",
    "                ent = np.dot(W_next.T, ent )\n",
    "                ent *= activations[ n ] * (1 - activations[ n ] ) # This is correct but confusing n+1 is n in the guide\n",
    "            else:  # NOTE: This assumes cross entropy cost\n",
    "                if self.actvfunc[ n ] == 'sigmoid':\n",
    "                    ent = ( activations[ n ] - y ) / y.shape[ 0 ]\n",
    "                elif self.actvfunc[ n ] == 'softmax':\n",
    "                    I = index2onehot( y , W.shape[ 0 ] )\n",
    "                    ent = (activations[ n ] - I ) / y.shape[ 0 ]\n",
    "\n",
    "            nabla_W = np.zeros( W.shape )\n",
    "            for l in np.arange( ent.shape[ 1 ] ):\n",
    "                if n == 0:\n",
    "                    nabla_W += np.outer( ent[ :, l ], x[ :, l] )\n",
    "                else:\n",
    "                    nabla_W += np.outer( ent[ :, l ], activations[ n - 1] [ :, l ] )\n",
    "            nabla_b = np.sum( ent , 1, keepdims=True )\n",
    "            \n",
    "            ########################\n",
    "            #End of the solution 5.1\n",
    "            ########################\n",
    "            \n",
    "            # Store the gradients\n",
    "            nabla_params[2*n] = nabla_W\n",
    "            nabla_params[2*n+1] = nabla_b\n",
    "\n",
    "        return nabla_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you are done. Try different network geometries by increasing the number of layers and layer sizes e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Neural network modules\n",
    "import lxmls.deep_learning.mlp as dl\n",
    "import lxmls.deep_learning.sgd as sgd\n",
    "# Model parameters\n",
    "geometry = [train_x.shape[0], 20, 2]\n",
    "actvfunc = ['sigmoid', 'softmax']\n",
    "# Instantiate model\n",
    "mlp = dl.NumpyMLP(geometry, actvfunc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can test the different hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 320/320 (100%)                                                                                                                                                                                                                                                                                                                              Epoch  1/ 5 in 1.81 seg\n",
      "Batch 320/320 (100%)                                                                                                                                                                                                                                                                                                                              Epoch  2/ 5 in 1.89 seg\n",
      "Batch 320/320 (100%)                                                                                                                                                                                                                                                                                                                              Epoch  3/ 5 in 1.76 seg\n",
      "Batch 320/320 (100%)                                                                                                                                                                                                                                                                                                                              Epoch  4/ 5 in 1.92 seg\n",
      "Batch 320/320 (100%)                                                                                                                                                                                                                                                                                                                              Epoch  5/ 5 in 1.82 seg\n",
      " \n",
      "MLP ([13989, 20, 2]) Amazon Sentiment Accuracy train: 0.964375 test: 0.780000\n"
     ]
    }
   ],
   "source": [
    "# Model parameters\n",
    "n_iter = 5\n",
    "bsize = 5\n",
    "lrate = 0.01\n",
    "# Train\n",
    "sgd.SGD_train(mlp, n_iter, bsize=bsize, lrate=lrate, train_set=(train_x, train_y))\n",
    "acc_train = sgd.class_acc(mlp.forward(train_x), train_y)[0]\n",
    "acc_test = sgd.class_acc(mlp.forward(test_x), test_y)[0]\n",
    "print \"MLP (%s) Amazon Sentiment Accuracy train: %f test: %f\" % (geometry, acc_train,acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.4.4 Some final reflections on Backpropagation**\n",
    "If you are new to the neural network topic, this is about the most important piece of theory you should learn\n",
    "about deep learning. Here are some reflections that you should keep in mind.\n",
    "* Thanks to the multi-layer structure and the chain rule, Backpropagation allows models that compose\n",
    "linear and non-linear functions with any depth (in principle5\n",
    ").\n",
    "* The formulas are also valid for other cost functions and output layer non-linearities with minor modifi-\n",
    "cations. It is only necessary to compute the equivalent of Eq. 5.13.\n",
    "* The formulas are also valid for hidden non-linearities other than the sigmoid. Element-wise non-linear\n",
    "transformations still allow the simplification in Eq. 5.21. With little effort it is also possible to deal with\n",
    "other cases.\n",
    "* However, there is an important limitation: unlike the log-linear models, the optimization problem is non\n",
    "convex. This removes some formal guarantees, most importantly we can get trapped in local minima\n",
    "during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Deriving gradients and GPU code with Theano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.5.1 An Introduction to Theano**\n",
    "As you may have observed, the speed of SGD training for MLPs slows down considerably when we increase the number of layers. One reason for this is that the code that we use here is not very optimized. It is thought for you to learn the basic principles. Even if the code was more optimized, it would still be very slow for reasonable network sizes. The cost of computing each linear layer is proportional to the dimensionality of the previous and current layers, which in most cases will be rather large.\n",
    "\n",
    "For this reason most deep learning applications use Graphics Processing Units (GPU) in their computations. This specialized hardware is normally used to accelerate computer graphics, but can also be used for general computation intensive tasks. However, we need to deal with specific interfaces and operations in order to use a GPU. This is where Theano comes in. Theano is a multidimensional symbolic expression python module\n",
    "with focus on neural networks. It will provide us with the following nice features:\n",
    "\n",
    "* Symbolic expressions: Express the operations of the MLP (forward pass, cost) symbolically, as mathematical operations rather than explicit code\n",
    "* Symbolic Differentiation: As a consequence of the previous feature, we can compute gradients of arbitrary mathematical functions automatically.\n",
    "* GPU integration: The code will be ready to work on a GPU, provided that you have one and it is active within Theano. It will also be faster on normal CPUs since the symbolic operations are compiled to C\n",
    "code.\n",
    "* Theano is focused on Deep Learning, with an active community and several tutorials easily available. However, this does not come at a free price. There are a number of limitations\n",
    "* Symbolic algebra is more difficult to debug, as we can not easily step in at each operation.\n",
    "* Working with CPU and GPU code implies that we have to be more careful about the types of the variables.\n",
    "* Theano tends to output long error messages. However, once you get used to it, error messages accurately point the source of the problem.\n",
    "* Handling recurrent neural networks is much simpler than in Numpy but it still implies working with complicated constructs that are complicated to debug."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5.2** Get in contact with Theano. Learn the difference between a symbolic representation and a function. Start\n",
    "by implementing the first layer of our previous MLP in Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Numpy code\n",
    "x = test_x # Test set\n",
    "W1, b1 = mlp.params[:2] # Weights and bias of fist layer\n",
    "z1 = np.dot(W1, x) + b1 # Linear transformation\n",
    "tilde_z1 = 1/(1+np.exp(-z1)) # Non-linear transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will implement this in Theano. We start by creating the variables over which we will produce the operations. For\n",
    "example the symbolic input is defined as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Theano code.\n",
    "# NOTE: We use undescore to denote symbolic equivalents to Numpy variables.\n",
    "# This is no Python convention!.\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "_x = T.matrix('x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_W1 = theano.shared(value=W1, name='W1', borrow=True)\n",
    "_b1 = theano.shared(value=b1, name='b1', borrow=True,broadcastable=(False, True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important:** One of the main differences between Numpy and theano data is broadcast. In Numpy if we sum an array\n",
    "with shape (N, M) to one with shape (1, M), the second array will be copied N times to form a (N, M) matrix. This is\n",
    "known as broadcasting. In Theano this is not automatic. You need to specify broadcasting explicitly. This is important\n",
    "for example when using a bias, which will be copied to match the number of examples in the batch. In other cases, like\n",
    "when using variables for recurrent neural networks, broadcast has to be set to False. Broadcast is one of the typical source of errors when you start working with Theano. Keep this in mind.\n",
    "Now lets describe the operations we want to do with the variables. Again only symbolically. This is done by replacing\n",
    "our usual operations by Theano symbolic ones when necessary e. g. the internal product dot() or the sigmoid. Some\n",
    "operations like e.g. + are automatically recognized by Theano (operator overloading)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_z1 = T.dot(_W1, _x) + _b1\n",
    "_tilde_z1 = T.nnet.sigmoid(_z1)\n",
    "# Keep in mind that naming variables is useful when debugging\n",
    "_z1.name = 'z1'\n",
    "_tilde_z1.name = 'tilde_z1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "This is my symbolic perceptron\n",
      "\n",
      "sigmoid [id A] 'tilde_z1'   \n",
      " |Elemwise{add,no_inplace} [id B] 'z1'   \n",
      "   |dot [id C] ''   \n",
      "   | |W1 [id D]\n",
      "   | |x [id E]\n",
      "   |b1 [id F]\n"
     ]
    }
   ],
   "source": [
    "# Show computation graph\n",
    "print \"\\nThis is my symbolic perceptron\\n\"\n",
    "theano.printing.debugprint(_tilde_z1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compile\n",
    "layer1 = theano.function([_x], _tilde_z1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Numpy and Theano Perceptrons are equivalent\n"
     ]
    }
   ],
   "source": [
    "# Check Numpy and Theano mactch\n",
    "if np.allclose(tilde_z1, layer1(x.astype(theano.config.floatX))):\n",
    "    print \"\\nNumpy and Theano Perceptrons are equivalent\"\n",
    "else:\n",
    "    set_trace()\n",
    "    # raise ValueError, \"Numpy and Theano Perceptrons are different\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5.2 Symbolic Forward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5.3** Complete the method forward() inside of the lxmls/deep learning/mlp.py:class TheanoMLP. Note that this is called only once at the initialization of the class. To debug your implementation put a breakpoint at the init function call. Hint: Note that this is very similar to NumpyMLP.forward(). You just need to keep track of the symbolic variable representing the output of the network after each layer is applied and compile the function at the end. After you are finished instantiate a Theano class and check that Numpy and Theano forward pass are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _forward(self, x, all_outputs=False):\n",
    "    \"\"\"\n",
    "    Symbolic forward pass\n",
    "\n",
    "    all_outputs = True  return symbolic input and intermediate activations\n",
    "    \"\"\"\n",
    "\n",
    "    # This will store activations at each layer and the input. This is\n",
    "    # needed to compute backpropagation\n",
    "    if all_outputs:\n",
    "        activations = [x]\n",
    "\n",
    "        # Input\n",
    "    tilde_z = x\n",
    "\n",
    "    ##########################\n",
    "    # Solution to Exercise 5.3\n",
    "    ##########################\n",
    "    for n in range(self.n_layers):\n",
    "\n",
    "        # Get weigths and bias (always in even and odd positions)\n",
    "        W = self.params[2*n]\n",
    "        b = self.params[2*n+1]\n",
    "\n",
    "        z = T.dot(W, tilde_z) + b # Linear transformation\n",
    "\n",
    "        # see e.g. theano.printing.debugprint(tilde_z)\n",
    "        z.name = 'z%d' % (n+1)\n",
    "\n",
    "        # Non-linear transformation\n",
    "        if self.actvfunc[n] == \"sigmoid\":\n",
    "            tilde_z = T.nnet.sigmoid( z )\n",
    "        elif self.actvfunc[n] == \"softmax\":\n",
    "            tilde_z = T.nnet.softmax( z.T ).T\n",
    "\n",
    "        tilde_z.name = 'tilde_z%d' % (n+1) # Name variable\n",
    "\n",
    "        if all_outputs:\n",
    "            activations.append(tilde_z)\n",
    "    #################################\n",
    "    # End of solution to Exercise 5.3\n",
    "    #################################\n",
    "\n",
    "    if all_outputs:\n",
    "        tilde_z = activations\n",
    "\n",
    "    return tilde_z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp_a = dl.NumpyMLP(geometry, actvfunc)\n",
    "mlp_b = dl.TheanoMLP(geometry, actvfunc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help debugging in Theano is sometimes useful to switch off the optimizer. This helps Theano point out which part\n",
    "of the Python code generated the error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theano.config.optimizer='None'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert np.allclose(mlp_a.forward(test_x), mlp_b.forward(test_x)),\"ERROR: Numpy and Theano forward passes differ\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5.3 Symbolic Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5.4** We first see an example that does not use any of the code in TheanoMLP but rather continues from what\n",
    "you wrote in Ex. 5.2. In this exercise you completed a sigmoid layer with Theano. To get some values for the weights\n",
    "we used the first layer of the network you trained in 5.2. Now we are going to use the second layer as well. This is thus assuming that your network in 5.2 has only two layers e.g. the recommended geometry (I, 20, 2). Make sure this is the case before starting this exercise.\n",
    "\n",
    "For the sake of clarity, lets write here the part of Ex. 5.2 that we had completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the values from our MLP\n",
    "W1, b1 = mlp.params[:2] # Weights and bias of fist layer\n",
    "# First layer symbolic variables\n",
    "_x = T.matrix('x')\n",
    "_W1 = theano.shared(value=W1, name='W1', borrow=True)\n",
    "_b1 = theano.shared(value=b1, name='b1', borrow=True, broadcastable=(False, True))\n",
    "# First layer symbolic expressions\n",
    "_z1 = T.dot(_W1, _x) + _b1\n",
    "_tilde_z1 = T.nnet.sigmoid(_z1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just need to complete this with the second layer, using a softmax non-linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W2, b2 = mlp.params[2:] # Weights and bias of second (and last!) layer\n",
    "# Second layer symbolic variables\n",
    "_W2 = theano.shared(value=W2, name='W2', borrow=True)\n",
    "_b2 = theano.shared(value=b2, name='b2', borrow=True, broadcastable=(False, True))\n",
    "# Second layer symbolic expressions\n",
    "_z2 = T.dot(_W2, _tilde_z1) + _b2\n",
    "# NOTE: Theano softmax does not support T.nnet.softmax(_z2, axis=1) this is a workaround\n",
    "_tilde_z2 = T.nnet.softmax(_z2.T).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, we could compile a function to obtain the output of the network symb tilde z2 for a given input symb x. In\n",
    "this exercise we are however interested in obtaining the misclassification cost. This is given in Eq: 5.5. First we are going\n",
    "to need the symbolic variable for the correct output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_y = T.ivector('y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The minus posterior probability of the class given the input is the same as selecting the k(m)-th softmax output, were\n",
    "k(m) is the index of the correct class for xm. If we want to do this for a vector y containing M different examples, we can\n",
    "write this as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_F = -T.mean(T.log(_tilde_z2[_y, T.arange(_y.shape[0])]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now obtaining a function that computes the gradient could not be easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_nabla_F = T.grad(_F, _W1)\n",
    "nabla_F = theano.function([_x, _y], _nabla_F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5.4 Symbolic mini-batch update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5.4** Define the updates list. This is a list where each element is a tuple of a parameter and the update rule to be\n",
    "applied that parameter. In this case we are defining the SGD update rule, but take into account that using more complex\n",
    "update rules like e.g. momentum or adam implies just replacing the last line of the following snippet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "This is my softmax classification cost\n",
      "\n",
      "Elemwise{neg,no_inplace} [id A] ''   \n",
      " |Elemwise{true_div,no_inplace} [id B] 'mean'   \n",
      "   |Sum{acc_dtype=float64} [id C] ''   \n",
      "   | |Elemwise{log,no_inplace} [id D] ''   \n",
      "   |   |AdvancedSubtensor [id E] ''   \n",
      "   |     |InplaceDimShuffle{1,0} [id F] ''   \n",
      "   |     | |Softmax [id G] ''   \n",
      "   |     |   |InplaceDimShuffle{1,0} [id H] ''   \n",
      "   |     |     |Elemwise{add,no_inplace} [id I] ''   \n",
      "   |     |       |dot [id J] ''   \n",
      "   |     |       | |W2 [id K]\n",
      "   |     |       | |sigmoid [id L] ''   \n",
      "   |     |       |   |Elemwise{add,no_inplace} [id M] ''   \n",
      "   |     |       |     |dot [id N] ''   \n",
      "   |     |       |     | |W1 [id O]\n",
      "   |     |       |     | |x [id P]\n",
      "   |     |       |     |b1 [id Q]\n",
      "   |     |       |b2 [id R]\n",
      "   |     |y [id S]\n",
      "   |     |ARange{dtype='int64'} [id T] ''   \n",
      "   |       |TensorConstant{0} [id U]\n",
      "   |       |Subtensor{int64} [id V] ''   \n",
      "   |       | |Shape [id W] ''   \n",
      "   |       | | |y [id S]\n",
      "   |       | |Constant{0} [id X]\n",
      "   |       |TensorConstant{1} [id Y]\n",
      "   |Subtensor{int64} [id Z] ''   \n",
      "     |Elemwise{Cast{float64}} [id BA] ''   \n",
      "     | |Shape [id BB] ''   \n",
      "     |   |Elemwise{log,no_inplace} [id D] ''   \n",
      "     |Constant{0} [id BC]\n"
     ]
    }
   ],
   "source": [
    "W2, b2 = mlp_a.params[2:4]\n",
    "\n",
    "# Second layer symbolic variables\n",
    "_W2 = theano.shared(value=W2, name='W2', borrow=True)\n",
    "_b2 = theano.shared(value=b2, name='b2', borrow=True,\n",
    "                    broadcastable=(False, True))\n",
    "_z2 = T.dot(_W2, _tilde_z1) + _b2\n",
    "_tilde_z2 = T.nnet.softmax(_z2.T).T\n",
    "\n",
    "# Ground truth\n",
    "_y = T.ivector('y')\n",
    "\n",
    "# Cost\n",
    "_F = -T.mean(T.log(_tilde_z2[_y, T.arange(_y.shape[0])]))\n",
    "\n",
    "# Gradient\n",
    "_nabla_F = T.grad(_F, _W1)\n",
    "nabla_F = theano.function([_x, _y], _nabla_F)\n",
    "\n",
    "# Print computation graph\n",
    "print \"\\nThis is my softmax classification cost\\n\"\n",
    "theano.printing.debugprint(_F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 320/320 (100%)                                                                                                                                                                                                                                                                                                                              Epoch  1/ 5 in 1.64 seg\n",
      "Batch 320/320 (100%)                                                                                                                                                                                                                                                                                                                              Epoch  2/ 5 in 1.55 seg\n",
      "Batch 320/320 (100%)                                                                                                                                                                                                                                                                                                                              Epoch  3/ 5 in 1.59 seg\n",
      "Batch 320/320 (100%)                                                                                                                                                                                                                                                                                                                              Epoch  4/ 5 in 1.64 seg\n",
      "Batch 320/320 (100%)                                                                                                                                                                                                                                                                                                                              Epoch  5/ 5 in 1.60 seg\n",
      " \n",
      "\n",
      "Numpy version took 8.02 sec\n",
      "Amazon Sentiment Accuracy train: 0.964375 test: 0.780000\n",
      "\n",
      "Batch 320/320 (100%)                                                                                                                                                                                                                                                                                                                              Epoch  1/ 5 in 1.80 seg\n",
      "Batch 320/320 (100%)                                                                                                                                                                                                                                                                                                                              Epoch  2/ 5 in 1.86 seg\n",
      "Batch 320/320 (100%)                                                                                                                                                                                                                                                                                                                              Epoch  3/ 5 in 1.88 seg\n",
      "Batch 320/320 (100%)                                                                                                                                                                                                                                                                                                                              Epoch  4/ 5 in 1.89 seg\n",
      "Batch 320/320 (100%)                                                                                                                                                                                                                                                                                                                              Epoch  5/ 5 in 1.89 seg\n",
      " \n",
      "\n",
      "Compiled gradient version took 9.32 sec\n",
      "Amazon Sentiment Accuracy train: 0.964375 test: 0.780000\n",
      "\n",
      "Batch 320/320 (100%)                                                                                                                                                                                                                                                                                                                              Epoch  1/ 5 in 0.92 seg\n",
      "Batch 320/320 (100%)                                                                                                                                                                                                                                                                                                                              Epoch  2/ 5 in 1.01 seg\n",
      "Batch 320/320 (100%)                                                                                                                                                                                                                                                                                                                              Epoch  3/ 5 in 0.99 seg\n",
      "Batch 320/320 (100%)                                                                                                                                                                                                                                                                                                                              Epoch  4/ 5 in 1.00 seg\n",
      "Batch 320/320 (100%)                                                                                                                                                                                                                                                                                                                              Epoch  5/ 5 in 1.01 seg\n",
      " \n",
      "\n",
      "Theano compiled batch update version took 4.93 sec\n",
      "Amazon Sentiment Accuracy train: 0.964375 test: 0.780000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Understanding the mini-batch function and givens/updates parameters\n",
    "\n",
    "# Numpy\n",
    "geometry = [train_x.shape[0], 20, 2]\n",
    "actvfunc = ['sigmoid', 'softmax']\n",
    "mlp_a = dl.NumpyMLP(geometry, actvfunc)\n",
    "#\n",
    "init_t = time.clock()\n",
    "sgd.SGD_train(mlp_a, n_iter, bsize=bsize, lrate=lrate, train_set=(train_x, train_y))\n",
    "print \"\\nNumpy version took %2.2f sec\" % (time.clock() - init_t)\n",
    "acc_train = sgd.class_acc(mlp_a.forward(train_x), train_y)[0]\n",
    "acc_test = sgd.class_acc(mlp_a.forward(test_x), test_y)[0]\n",
    "print \"Amazon Sentiment Accuracy train: %f test: %f\\n\" % (acc_train, acc_test)\n",
    "\n",
    "# Theano grads\n",
    "mlp_b = dl.TheanoMLP(geometry, actvfunc)\n",
    "init_t = time.clock()\n",
    "sgd.SGD_train(mlp_b, n_iter, bsize=bsize, lrate=lrate, train_set=(train_x, train_y))\n",
    "print \"\\nCompiled gradient version took %2.2f sec\" % (time.clock() - init_t)\n",
    "acc_train = sgd.class_acc(mlp_b.forward(train_x), train_y)[0]\n",
    "acc_test = sgd.class_acc(mlp_b.forward(test_x), test_y)[0]\n",
    "print \"Amazon Sentiment Accuracy train: %f test: %f\\n\" % (acc_train, acc_test)\n",
    "\n",
    "# Theano compiled batch\n",
    "\n",
    "# Cast data into the types and shapes used in the theano graph\n",
    "# IMPORTANT: This is the main source of errors when beginning with theano\n",
    "train_x = train_x.astype(theano.config.floatX)\n",
    "train_y = train_y.astype('int32')\n",
    "\n",
    "# Model\n",
    "mlp_c = dl.TheanoMLP(geometry, actvfunc)\n",
    "\n",
    "# Define givens variables to be used in the batch update\n",
    "# Get symbolic variables returning a mini-batch of data\n",
    "\n",
    "# Define updates variable. This is a list of gradient descent updates\n",
    "# The output is a list following theano.function updates parameter. This\n",
    "# consists on a list of tuples with each parameter and update rule\n",
    "_x = T.matrix('x')\n",
    "_y = T.ivector('y')\n",
    "_F = mlp_c._cost(_x, _y)\n",
    "updates = [(par, par - lrate*T.grad(_F, par)) for par in mlp_c.params]\n",
    "\n",
    "#\n",
    "# Define the batch update function. This will return the cost of each batch\n",
    "# and update the MLP parameters at the same time using updates\n",
    "batch_up = theano.function([_x, _y], _F, updates=updates)\n",
    "n_batch = int(np.ceil(float(train_x.shape[1])/bsize)) \n",
    "#\n",
    "\n",
    "init_t = time.clock()\n",
    "sgd.SGD_train(mlp_c, n_iter, batch_up=batch_up, n_batch=n_batch, bsize=bsize,\n",
    "              train_set=(train_x, train_y))\n",
    "print \"\\nTheano compiled batch update version took %2.2f sec\" % (time.clock() - init_t)\n",
    "init_t = time.clock()\n",
    "\n",
    "acc_train = sgd.class_acc(mlp_c.forward(train_x), train_y)[0]\n",
    "acc_test = sgd.class_acc(mlp_c.forward(test_x), test_y)[0]\n",
    "print \"Amazon Sentiment Accuracy train: %f test: %f\\n\" % (acc_train, acc_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
