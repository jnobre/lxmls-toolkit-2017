{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LXMLS 2017 - Day 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Structed Predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this class, we will continue to focus on sequence classification, but instead of following a generative approach (like in the previous chapter) we move towards discriminative approaches. Recall that the difference between these approaches is that generative approaches attempt to model the probability distribution of the data, P(X,Y), whereas discriminative ones only model the conditional probability of the sequence, given the observed data, P(Y|X)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Today’s Assignment**\n",
    "The assignment of today’s class is to implement the structured version of the perceptron algorithm.\n",
    "\n",
    "** 3.1 Classification vs Sequential Classification **\n",
    "Table 3.1 shows how the models for classification have counterparts in sequential classification. In fact, in the last chapter we discussed the Hidden Markov model, which can be seen as a generalization of the Na¨ıve Bayes model for sequences. In this chapter, we will see a generalization of the Perceptron algorithm for sequence problems (yielding the Structured Perceptron, Collins 2002) and a generalization of Maximum Entropy model for sequences (yielding Conditional Random Fields, Lafferty et al. 2001). Note that both these generalizations are not specific for sequences and can be applied to a wide range of models (we will see in tomorrow’s lecture how these methods can be applied to parsing). Although we will not cover all the methods described in Chapter 1, bear in mind that all of those have a structured counterpart. It should be intuitive after this lecture how those methods could be extended to structured problems, given the perceptron example. \n",
    "\n",
    "See **Pag 66**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.1** In this exercise you will train a CRF using different feature sets for Part-of-Speech Tagging. Start with the\n",
    "code below, which uses the ID feature set from table 3.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRF Exercise\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../../../')\n",
    "\n",
    "import lxmls.sequences.crf_online as crfo\n",
    "import lxmls.sequences.structured_perceptron as spc\n",
    "import lxmls.readers.pos_corpus as pcc\n",
    "import lxmls.sequences.id_feature as idfc\n",
    "import lxmls.sequences.extended_feature as exfc\n",
    "\n",
    "print \"CRF Exercise\"\n",
    "\n",
    "corpus = pcc.PostagCorpus( )\n",
    "train_seq = corpus.read_sequence_list_conll(\"../../../data/train-02-21.conll\", max_sent_len=10, max_nr_sent=1000)\n",
    "test_seq = corpus.read_sequence_list_conll(\"../../../data/test-23.conll\", max_sent_len=10, max_nr_sent=1000)\n",
    "dev_seq = corpus.read_sequence_list_conll(\"../../../data/dev-22.conll\", max_sent_len=10, max_nr_sent=1000)\n",
    "\n",
    "feature_mapper = idfc.IDFeatures(train_seq)\n",
    "feature_mapper.build_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Objective value: -5.779018\n",
      "Epoch: 1 Objective value: -3.192724\n",
      "Epoch: 2 Objective value: -2.717537\n",
      "Epoch: 3 Objective value: -2.436614\n",
      "Epoch: 4 Objective value: -2.240491\n",
      "Epoch: 5 Objective value: -2.091833\n",
      "Epoch: 6 Objective value: -1.973353\n",
      "Epoch: 7 Objective value: -1.875643\n",
      "Epoch: 8 Objective value: -1.793034\n",
      "Epoch: 9 Objective value: -1.721857\n",
      "Epoch: 10 Objective value: -1.659605\n",
      "Epoch: 11 Objective value: -1.604499\n",
      "Epoch: 12 Objective value: -1.555229\n",
      "Epoch: 13 Objective value: -1.510806\n",
      "Epoch: 14 Objective value: -1.470468\n",
      "Epoch: 15 Objective value: -1.433612\n",
      "Epoch: 16 Objective value: -1.399759\n",
      "Epoch: 17 Objective value: -1.368518\n",
      "Epoch: 18 Objective value: -1.339566\n",
      "Epoch: 19 Objective value: -1.312636\n",
      "CRF - ID Features Accuracy Train: 0.949 Dev: 0.846 Test: 0.858\n"
     ]
    }
   ],
   "source": [
    "crf_online = crfo.CRFOnline(corpus.word_dict, corpus.tag_dict, feature_mapper)\n",
    "crf_online.num_epochs = 20\n",
    "crf_online.train_supervised(train_seq)\n",
    "\n",
    "pred_train = crf_online.viterbi_decode_corpus(train_seq)\n",
    "pred_dev = crf_online.viterbi_decode_corpus(dev_seq)\n",
    "pred_test = crf_online.viterbi_decode_corpus(test_seq)\n",
    "eval_train = crf_online.evaluate_corpus(train_seq, pred_train)\n",
    "eval_dev = crf_online.evaluate_corpus(dev_seq, pred_dev)\n",
    "eval_test = crf_online.evaluate_corpus(test_seq, pred_test)\n",
    "\n",
    "print \"CRF - ID Features Accuracy Train: %.3f Dev: %.3f Test: %.3f\" % (eval_train, eval_dev, eval_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.2** Repeat the previous exercise using the extended feature set. Compare the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Objective value: -7.141596\n",
      "Epoch: 1 Objective value: -1.807511\n",
      "Epoch: 2 Objective value: -1.218877\n",
      "Epoch: 3 Objective value: -0.955739\n",
      "Epoch: 4 Objective value: -0.807821\n",
      "Epoch: 5 Objective value: -0.712858\n",
      "Epoch: 6 Objective value: -0.647382\n",
      "Epoch: 7 Objective value: -0.599442\n",
      "Epoch: 8 Objective value: -0.562584\n",
      "Epoch: 9 Objective value: -0.533411\n",
      "Epoch: 10 Objective value: -0.509885\n",
      "Epoch: 11 Objective value: -0.490548\n",
      "Epoch: 12 Objective value: -0.474318\n",
      "Epoch: 13 Objective value: -0.460438\n",
      "Epoch: 14 Objective value: -0.448389\n",
      "Epoch: 15 Objective value: -0.437800\n",
      "Epoch: 16 Objective value: -0.428402\n",
      "Epoch: 17 Objective value: -0.419990\n",
      "Epoch: 18 Objective value: -0.412406\n",
      "Epoch: 19 Objective value: -0.405524\n",
      "CRF - Extended Features Accuracy Train: 0.984 Dev: 0.899 Test: 0.894\n"
     ]
    }
   ],
   "source": [
    "feature_mapper = exfc.ExtendedFeatures(train_seq)\n",
    "feature_mapper.build_features()\n",
    "\n",
    "crf_online = crfo.CRFOnline(corpus.word_dict, corpus.tag_dict, feature_mapper)\n",
    "crf_online.num_epochs = 20\n",
    "crf_online.train_supervised(train_seq)\n",
    "\n",
    "pred_train = crf_online.viterbi_decode_corpus(train_seq)\n",
    "pred_dev = crf_online.viterbi_decode_corpus(dev_seq)\n",
    "pred_test = crf_online.viterbi_decode_corpus(test_seq)\n",
    "eval_train = crf_online.evaluate_corpus(train_seq, pred_train)\n",
    "eval_dev = crf_online.evaluate_corpus(dev_seq, pred_dev)\n",
    "eval_test = crf_online.evaluate_corpus(test_seq, pred_test)\n",
    "\n",
    "print \"CRF - Extended Features Accuracy Train: %.3f Dev: %.3f Test: %.3f\" % (eval_train, eval_dev, eval_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.5 Structured Perceptron**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structured perceptron (Collins, 2002), namely its averaged version, is a very simple algorithm that relies on Viterbi decoding and very simple additive updates. In practice this algorithm is very easy to implement and behaves remarkably well in a variety of problems. These two characteristics make the structured perceptron\n",
    "algorithm a natural first choice to try and test a new problem or a new feature set. Recall what you learned about the perceptron algorithm (Section 1.4.3) and compare it against the structured\n",
    "perceptron (Algorithm 11)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.3** Implement the structured perceptron algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def perceptron_update(self, sequence):\n",
    "\n",
    "    ##########################\n",
    "    # Solution to Exercise 3.3\n",
    "    ##########################\n",
    "\n",
    "    num_labels = 0\n",
    "    num_mistakes = 0\n",
    "\n",
    "    predicted_sequence, _ = self.viterbi_decode(sequence)\n",
    "\n",
    "    y_hat = predicted_sequence.y\n",
    "\n",
    "    # Update initial features.\n",
    "    y_t_true = sequence.y[0]\n",
    "    y_t_hat = y_hat[0]\n",
    "\n",
    "    if y_t_true != y_t_hat:\n",
    "        true_initial_features = self.feature_mapper.get_initial_features(sequence, y_t_true)\n",
    "        self.parameters[true_initial_features] += self.learning_rate\n",
    "        hat_initial_features = self.feature_mapper.get_initial_features(sequence, y_t_hat)\n",
    "        self.parameters[hat_initial_features] -= self.learning_rate\n",
    "\n",
    "    for pos in xrange(len(sequence.x)):\n",
    "        y_t_true = sequence.y[pos]\n",
    "        y_t_hat = y_hat[pos]\n",
    "\n",
    "        # Update emission features.\n",
    "        num_labels += 1\n",
    "        if y_t_true != y_t_hat:\n",
    "            num_mistakes += 1\n",
    "            true_emission_features = self.feature_mapper.get_emission_features(sequence, pos, y_t_true)\n",
    "            self.parameters[true_emission_features] += self.learning_rate\n",
    "            hat_emission_features = self.feature_mapper.get_emission_features(sequence, pos, y_t_hat)\n",
    "            self.parameters[hat_emission_features] -= self.learning_rate\n",
    "\n",
    "        if pos > 0:\n",
    "            # update bigram features\n",
    "            # If true bigram != predicted bigram update bigram features\n",
    "            prev_y_t_true = sequence.y[pos-1]\n",
    "            prev_y_t_hat = y_hat[pos-1]\n",
    "            if y_t_true != y_t_hat or prev_y_t_true != prev_y_t_hat:\n",
    "                true_transition_features = self.feature_mapper.get_transition_features(\n",
    "                    sequence, pos-1, y_t_true, prev_y_t_true)\n",
    "                self.parameters[true_transition_features] += self.learning_rate\n",
    "                hat_transition_features = self.feature_mapper.get_transition_features(\n",
    "                    sequence, pos-1, y_t_hat, prev_y_t_hat)\n",
    "                self.parameters[hat_transition_features] -= self.learning_rate\n",
    "\n",
    "    pos = len(sequence.x)\n",
    "    y_t_true = sequence.y[pos-1]\n",
    "    y_t_hat = y_hat[pos-1]\n",
    "\n",
    "    if y_t_true != y_t_hat:\n",
    "        true_final_features = self.feature_mapper.get_final_features(sequence, y_t_true)\n",
    "        self.parameters[true_final_features] += self.learning_rate\n",
    "        hat_final_features = self.feature_mapper.get_final_features(sequence, y_t_hat)\n",
    "        self.parameters[hat_final_features] -= self.learning_rate\n",
    "\n",
    "    return num_labels, num_mistakes\n",
    "\n",
    "    #################################\n",
    "    # End of solution to Exercise 3.3\n",
    "    #################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.4** Repeat Exercises 3.1–3.2 using the structured perceptron algorithm instead of a CRF. Report the results.\n",
    "Here is the code for the simple feature set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron Exercise\n",
      "Epoch: 0 Accuracy: 0.656806\n",
      "Epoch: 1 Accuracy: 0.820898\n",
      "Epoch: 2 Accuracy: 0.879176\n",
      "Epoch: 3 Accuracy: 0.907432\n",
      "Epoch: 4 Accuracy: 0.925239\n",
      "Epoch: 5 Accuracy: 0.939956\n",
      "Epoch: 6 Accuracy: 0.946284\n",
      "Epoch: 7 Accuracy: 0.953790\n",
      "Epoch: 8 Accuracy: 0.958499\n",
      "Epoch: 9 Accuracy: 0.955114\n",
      "Epoch: 10 Accuracy: 0.959235\n",
      "Epoch: 11 Accuracy: 0.968065\n",
      "Epoch: 12 Accuracy: 0.968212\n",
      "Epoch: 13 Accuracy: 0.966740\n",
      "Epoch: 14 Accuracy: 0.971302\n",
      "Epoch: 15 Accuracy: 0.968653\n",
      "Epoch: 16 Accuracy: 0.970419\n",
      "Epoch: 17 Accuracy: 0.971891\n",
      "Epoch: 18 Accuracy: 0.971744\n",
      "Epoch: 19 Accuracy: 0.973510\n",
      "Structured Perceptron - ID Features Accuracy Train: 0.984 Dev: 0.835 Test: 0.840\n",
      "Epoch: 0 Accuracy: 0.764386\n",
      "Epoch: 1 Accuracy: 0.872701\n",
      "Epoch: 2 Accuracy: 0.903458\n",
      "Epoch: 3 Accuracy: 0.927594\n",
      "Epoch: 4 Accuracy: 0.938484\n",
      "Epoch: 5 Accuracy: 0.951141\n",
      "Epoch: 6 Accuracy: 0.949816\n",
      "Epoch: 7 Accuracy: 0.959529\n",
      "Epoch: 8 Accuracy: 0.957616\n",
      "Epoch: 9 Accuracy: 0.962325\n",
      "Epoch: 10 Accuracy: 0.961148\n",
      "Epoch: 11 Accuracy: 0.970567\n",
      "Epoch: 12 Accuracy: 0.968212\n",
      "Epoch: 13 Accuracy: 0.973216\n",
      "Epoch: 14 Accuracy: 0.974393\n",
      "Epoch: 15 Accuracy: 0.973951\n",
      "Epoch: 16 Accuracy: 0.976600\n",
      "Epoch: 17 Accuracy: 0.977483\n",
      "Epoch: 18 Accuracy: 0.974834\n",
      "Epoch: 19 Accuracy: 0.977042\n",
      "Structured Perceptron - Extended Features Accuracy Train: 0.984 Dev: 0.888 Test: 0.890\n"
     ]
    }
   ],
   "source": [
    "print \"Perceptron Exercise\"\n",
    "\n",
    "feature_mapper = idfc.IDFeatures(train_seq)\n",
    "feature_mapper.build_features()\n",
    "\n",
    "sp = spc.StructuredPerceptron(corpus.word_dict, corpus.tag_dict, feature_mapper)\n",
    "sp.num_epochs = 20\n",
    "sp.train_supervised(train_seq)\n",
    "\n",
    "pred_train = sp.viterbi_decode_corpus(train_seq)\n",
    "pred_dev = sp.viterbi_decode_corpus(dev_seq)\n",
    "pred_test = sp.viterbi_decode_corpus(test_seq)\n",
    "eval_train = sp.evaluate_corpus(train_seq, pred_train)\n",
    "eval_dev = sp.evaluate_corpus(dev_seq, pred_dev)\n",
    "eval_test = sp.evaluate_corpus(test_seq, pred_test)\n",
    "\n",
    "print \"Structured Perceptron - ID Features Accuracy Train: %.3f Dev: %.3f Test: %.3f\" % (eval_train, eval_dev, eval_test)\n",
    "\n",
    "feature_mapper = exfc.ExtendedFeatures(train_seq)\n",
    "feature_mapper.build_features()\n",
    "\n",
    "sp = spc.StructuredPerceptron(corpus.word_dict, corpus.tag_dict, feature_mapper)\n",
    "sp.num_epochs = 20\n",
    "sp.train_supervised(train_seq)\n",
    "\n",
    "pred_train = sp.viterbi_decode_corpus(train_seq)\n",
    "pred_dev = sp.viterbi_decode_corpus(dev_seq)\n",
    "pred_test = sp.viterbi_decode_corpus(test_seq)\n",
    "eval_train = sp.evaluate_corpus(train_seq, pred_train)\n",
    "eval_dev = sp.evaluate_corpus(dev_seq, pred_dev)\n",
    "eval_test = sp.evaluate_corpus(test_seq, pred_test)\n",
    "\n",
    "print \"Structured Perceptron - Extended Features Accuracy Train: %.3f Dev: %.3f Test: %.3f\" % (eval_train, eval_dev, eval_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
